# https://raw.githubusercontent.com/longhorn/longhorn/master/chart/values.yaml

longhorn:
  defaultSettings:
    # -- The endpoint used to access the backupstore. Available: NFS, CIFS, AWS, GCP, AZURE.
    backupTarget: 'nfs://mercury.local:/volume1/longhorn'
    # -- The name of the Kubernetes secret associated with the backup target.
    backupTargetCredentialSecret: ~
    # -- If this setting is enabled, Longhorn will automatically attach the volume and takes snapshot/backup
    # when it is the time to do recurring snapshot/backup.
    allowRecurringJobWhileVolumeDetached: ~
    # -- Create default Disk automatically only on Nodes with the label "node.longhorn.io/create-default-disk=true" if no other disks exist.
    # If disabled, the default disk will be created on all new nodes when each node is first added.
    createDefaultDiskLabeledNodes: ~
    # -- Default path to use for storing data on a host. By default, "/var/lib/longhorn/"
    defaultDataPath: ~
    # -- Longhorn volume has data locality if there is a local replica of the volume on the same node as the pod which is using the volume.
    defaultDataLocality: ~
    # -- Allow scheduling on nodes with existing healthy replicas of the same volume. By default, false.
    replicaSoftAntiAffinity: ~
    # -- Enable this setting automatically re-balances replicas when discovered an available node.
    replicaAutoBalance: ~
    # -- Percentage of storage that can be allocated relative to hard drive capacity. The default value is 100.
    storageOverProvisioningPercentage: ~
    # -- If the minimum available disk capacity exceeds the actual percentage of available disk capacity,
    # the disk becomes unschedulable until more space is freed up. By default, 25.
    storageMinimalAvailablePercentage: ~
    # -- The reserved percentage specifies the percentage of disk space that will not be allocated to the default disk on each new Longhorn node.
    storageReservedPercentageForDefaultDisk: ~
    # -- Upgrade Checker will check for a new Longhorn version periodically.
    # When there is a new version available, a notification will appear in the UI. By default, true.
    upgradeChecker: ~
    # -- The default number of replicas when a volume is created from the Longhorn UI.
    # For Kubernetes configuration, update the `numberOfReplicas` in the StorageClass. By default, 3.
    defaultReplicaCount: ~
    # -- The 'storageClassName' is given to PVs and PVCs that are created for an existing Longhorn volume. The StorageClass name can also be used as a label,
    # so it is possible to use a Longhorn StorageClass to bind a workload to an existing PV without creating a Kubernetes StorageClass object.
    # By default, 'longhorn-static'.
    defaultLonghornStaticStorageClass: ~
    # -- In seconds. The backup store poll interval determines how often Longhorn checks the backup store for new backups.
    # Set to 0 to disable the polling. By default, 300.
    backupstorePollInterval: ~
    # -- In minutes. This setting determines how long Longhorn will keep the backup resource that was failed. Set to 0 to disable the auto-deletion.
    failedBackupTTL: ~
    # -- Restore recurring jobs from the backup volume on the backup target and create recurring jobs if not exist during a backup restoration.
    restoreVolumeRecurringJobs: ~
    # -- This setting specifies how many successful backup or snapshot job histories should be retained. History will not be retained if the value is 0.
    recurringSuccessfulJobsHistoryLimit: ~
    # -- This setting specifies how many failed backup or snapshot job histories should be retained. History will not be retained if the value is 0.
    recurringFailedJobsHistoryLimit: ~
    # -- Maximum number of snapshots or backups to be retained.
    recurringJobMaxRetention: ~
    # -- This setting specifies how many failed support bundles can exist in the cluster.
    # Set this value to **0** to have Longhorn automatically purge all failed support bundles.
    supportBundleFailedHistoryLimit: ~
    # -- taintToleration for Longhorn system-managed components
    taintToleration: ~
    # -- nodeSelector for Longhorn system-managed components
    systemManagedComponentsNodeSelector: ~
    # -- priorityClass for Longhorn system-managed components
    # This setting can help prevent Longhorn components from being evicted under Node Pressure.
    # Notice that this will be applied to Longhorn user-deployed components by default if there are no priority class values set yet, such as `longhornManager.priorityClass`.
    priorityClass: &defaultPriorityClassNameRef 'longhorn-critical'
    # -- If enabled, volumes will be automatically salvaged when all the replicas become faulty, e.g., due to network disconnection.
    # Longhorn will try to figure out which replica(s) are usable, then use them for the volume. By default, true.
    autoSalvage: ~
    # -- If enabled, Longhorn will automatically delete the workload pod managed by a controller (e.g., Deployment, StatefulSet, DaemonSet, etc...)
    # when Longhorn volume is detached unexpectedly (e.g., during Kubernetes upgrade, Docker reboot, or network disconnect).
    # By deleting the pod, its controller restarts the pod and Kubernetes handles volume reattachment and remount.
    autoDeletePodWhenVolumeDetachedUnexpectedly: ~
    # -- Disable Longhorn manager to schedule replica on Kubernetes cordoned node. By default, true.
    disableSchedulingOnCordonedNode: ~
    # -- Allow scheduling new Replicas of Volume to the Nodes in the same Zone as existing healthy Replicas.
    # Nodes don't belong to any Zone will be treated as in the same Zone.
    # Notice that Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
    # By default, true.
    replicaZoneSoftAntiAffinity: ~
    # -- Allow scheduling on disks with existing healthy replicas of the same volume. By default, true.
    replicaDiskSoftAntiAffinity: ~
    # -- Defines the Longhorn action when a Volume is stuck with a StatefulSet/Deployment Pod on a node that is down.
    nodeDownPodDeletionPolicy: delete-both-statefulset-and-deployment-pod
    # -- Define the policy to use when a node with the last healthy replica of a volume is drained.
    nodeDrainPolicy: ~
    # -- Automatically detach volumes that are attached manually when the node is cordoned.
    detachManuallyAttachedVolumesWhenCordoned: ~
    # -- The interval in seconds determines how long Longhorn will at least wait to reuse the existing data on a failed replica
    # rather than directly creating a new replica for a degraded volume.
    replicaReplenishmentWaitInterval: ~
    # -- This setting controls how many replicas on a node can be rebuilt simultaneously.
    concurrentReplicaRebuildPerNodeLimit: ~
    # -- This setting controls how many volumes on a node can restore the backup concurrently. Set the value to **0** to disable backup restore.
    concurrentVolumeBackupRestorePerNodeLimit: ~
    # -- This setting is only for volumes created by UI.
    # By default, this is false, meaning there will be a revision counter file to track every write to the volume.
    # During salvage-recovering, Longhorn will pick the replica with the largest revision counter as a candidate to recover the whole volume.
    # If the revision counter is disabled, Longhorn will not track every write to the volume.
    # During the salvage recovering, Longhorn will use the 'volume-head-xxx.img' file last modification time and
    # file size to pick the replica candidate to recover the whole volume.
    disableRevisionCounter: ~
    # -- This setting defines the Image Pull Policy of Longhorn system managed pod.
    # E.g. instance manager, engine image, CSI driver, etc.
    # The new Image Pull Policy will only apply after the system-managed pods restart.
    systemManagedPodsImagePullPolicy: ~
    # -- This setting allows user to create and attach a volume that doesn't have all the replicas scheduled at the time of creation.
    allowVolumeCreationWithDegradedAvailability: ~
    # -- This setting enables Longhorn to automatically clean up the system-generated snapshot after replica rebuild is done.
    autoCleanupSystemGeneratedSnapshot: ~
    # -- This setting enables Longhorn to automatically cleanup the snapshot generated by a recurring backup job.
    autoCleanupRecurringJobBackupSnapshot: ~
    # -- This setting controls how Longhorn automatically upgrades volumes' engines to the new default engine image after upgrading Longhorn manager.
    # The value of this setting specifies the maximum number of engines per node that are allowed to upgrade to the default engine image at the same time.
    # If the value is 0, Longhorn will not automatically upgrade volumes' engines to the default version of engine image.
    concurrentAutomaticEngineUpgradePerNodeLimit: ~
    # -- This interval in minutes determines how long Longhorn will wait before cleaning up the backing image file when there is no replica in the disk using it.
    backingImageCleanupWaitInterval: ~
    # -- This interval in seconds determines how long Longhorn will wait before re-downloading the backing image file
    # when all disk files of this backing image become failed or unknown.
    backingImageRecoveryWaitInterval: ~
    # -- Percentage of the total allocatable CPU resources on each node to be reserved for each instance manager pod when the V1 Data Engine is enabled.
    # The default value is 12 percent.
    guaranteedInstanceManagerCPU: ~
    # -- Setting that notifies Longhorn that the cluster is using the Kubernetes Cluster Autoscaler.
    kubernetesClusterAutoscalerEnabled: ~
    # -- This setting allows Longhorn to delete the orphan resource and its corresponding orphaned data automatically like stale replicas.
    # Orphan resources on down or unknown nodes will not be cleaned up automatically.
    orphanAutoDeletion: ~
    # -- Longhorn uses the storage network for in-cluster data traffic. Leave this blank to use the Kubernetes cluster network.
    storageNetwork: ~
    # -- This flag is designed to prevent Longhorn from being accidentally uninstalled which will lead to data lost.
    deletingConfirmationFlag: ~
    # -- In seconds. The setting specifies the timeout between the engine and replica(s), and the value should be between 8 and 30 seconds.
    # The default value is 8 seconds.
    engineReplicaTimeout: ~
    # -- This setting allows users to enable or disable snapshot hashing and data integrity checking.
    snapshotDataIntegrity: ~
    # -- Hashing snapshot disk files impacts the performance of the system.
    # The immediate snapshot hashing and checking can be disabled to minimize the impact after creating a snapshot.
    snapshotDataIntegrityImmediateCheckAfterSnapshotCreation: ~
    # -- Unix-cron string format. The setting specifies when Longhorn checks the data integrity of snapshot disk files.
    snapshotDataIntegrityCronjob: ~
    # -- This setting allows Longhorn filesystem trim feature to automatically mark the latest snapshot and
    # its ancestors as removed and stops at the snapshot containing multiple children.
    removeSnapshotsDuringFilesystemTrim: ~
    # -- This feature supports the fast replica rebuilding.
    # It relies on the checksum of snapshot disk files, so setting the snapshot-data-integrity to **enable** or **fast-check** is a prerequisite.
    fastReplicaRebuildEnabled: ~
    # -- In seconds. The setting specifies the HTTP client timeout to the file sync server.
    replicaFileSyncHttpClientTimeout: ~
    # -- The log level Panic, Fatal, Error, Warn, Info, Debug, Trace used in longhorn manager. Default to Info.
    logLevel: ~
    # -- This setting allows users to specify backup compression method.
    backupCompressionMethod: ~
    # -- This setting controls how many worker threads per backup concurrently.
    backupConcurrentLimit: ~
    # -- This setting controls how many worker threads per restore concurrently.
    restoreConcurrentLimit: ~
    # -- Setting that allows you to enable the V1 Data Engine. This setting is enabled by default.
    v1DataEngine: ~
    # -- Setting that allows you to enable the V2 Data Engine, which is based on the Storage Performance Development Kit (SPDK).
    # The V2 Data Engine is a preview feature and should not be used in production environments.
    v2DataEngine: ~
    # -- This setting allows users to enable the offline replica rebuilding for volumes using v2 data engine.
    offlineReplicaRebuilding: ~
    # -- Number of millicpus on each node to be reserved for each instance manager pod when the V2 Data Engine is enabled.
    # The default value is 1250 millicpus.
    v2DataEngineGuaranteedInstanceManagerCPU: ~
    # -- Allow Scheduling Empty Node Selector Volumes To Any Node
    allowEmptyNodeSelectorVolume: ~
    # -- Allow Scheduling Empty Disk Selector Volumes To Any Disk
    allowEmptyDiskSelectorVolume: ~
    # -- Enabling this setting will allow Longhorn to provide additional usage metrics to https://metrics.longhorn.io/. This information will help us better understand how Longhorn is being used, which will ultimately contribute to future improvements.
    allowCollectingLonghornUsageMetrics: ~
    # -- Temporarily prevent all attempts to purge volume snapshots.
    disableSnapshotPurge: ~

  persistence:
    # If you have three or more nodes for storage, use 3; otherwise use 2
    defaultClassReplicaCount: 2

  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: mekanics-ca-issuer
      hajimari.io/group: System
      hajimari.io/appName: Longhorn
      hajimari.io/icon: cow
    host: &host longhorn.system.mekanics.ch
    tlsSecret: longhorn-tls-certificate
